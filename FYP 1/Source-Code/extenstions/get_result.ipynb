{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "# import dlib\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Svhkpt wpckpn,'w'kumpds,\n",
    "#FIEPTH:/://MPD/pyb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headings = [\"image-Path\", \"YOlov8\", \"opencv\", \"dlib\"]\n",
    "with open('face_detection_results.csv', mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the column headings\n",
    "    writer.writerow(column_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\face_detection\\MP_Data\\input_images\\18K1196\\Beard\\left-to-right\\1\\0008_2.jpg: 480x640 4 persons, 3 cars, 2 backpacks, 1175.7ms\n",
      "Speed: 2.0ms preprocess, 1175.7ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "1\n",
      "\n",
      "image 1/1 d:\\face_detection\\MP_Data\\input_images\\18K1196\\Beard\\left-to-right\\1\\0009_2.jpg: 480x640 4 persons, 3 cars, 2 backpacks, 1150.5ms\n",
      "Speed: 2.0ms preprocess, 1150.5ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "2\n",
      "\n",
      "image 1/1 d:\\face_detection\\MP_Data\\input_images\\18K1196\\Beard\\left-to-right\\1\\0010_2.jpg: 480x640 4 persons, 3 cars, 2 backpacks, 1117.6ms\n",
      "Speed: 2.0ms preprocess, 1117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "3\n",
      "\n",
      "image 1/1 d:\\face_detection\\MP_Data\\input_images\\18K1196\\Beard\\left-to-right\\1\\0011_2.jpg: 480x640 4 persons, 4 cars, 4 backpacks, 1223.1ms\n",
      "Speed: 6.0ms preprocess, 1223.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "4\n",
      "\n",
      "image 1/1 d:\\face_detection\\MP_Data\\input_images\\18K1196\\Beard\\left-to-right\\1\\0012_2.jpg: 480x640 4 persons, 4 cars, 3 backpacks, 1309.6ms\n",
      "Speed: 2.5ms preprocess, 1309.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m checkpoint_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_faces.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[43msave_cropped_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 60\u001b[0m, in \u001b[0;36msave_cropped_faces\u001b[1;34m(input_dir, checkpoint_file)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relative_path \u001b[38;5;129;01min\u001b[39;00m processed_images:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m yolo \u001b[38;5;241m=\u001b[39m \u001b[43muse_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m opencv \u001b[38;5;241m=\u001b[39m use_opencv(img_path)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# dlib = use_dlib(img_path)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Update processed images set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 5\u001b[0m, in \u001b[0;36muse_yolo\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muse_yolo\u001b[39m(img_path):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Assume model_yolov8 is defined and loaded somewhere in your code\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     model_yolov8\u001b[38;5;241m=\u001b[39mYOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./yolov8l.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_yolov8\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Access bounding boxes directly (assuming YOLOv8 returns a list)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     faces \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:98\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:232\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m'\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\predictor.py:317\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:123\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nn_module:  \u001b[38;5;66;03m# in-memory PyTorch model\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 123\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m model\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    125\u001b[0m         kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\nn\\tasks.py:129\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[0;32m    128\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[1;32m--> 129\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[0;32m    131\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\utils\\torch_utils.py:170\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[1;34m(conv, bn)\u001b[0m\n\u001b[0;32m    160\u001b[0m fusedconv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(conv\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[0;32m    161\u001b[0m                       conv\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[0;32m    162\u001b[0m                       kernel_size\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m                       groups\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    167\u001b[0m                       bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Prepare filters\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m w_conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    171\u001b[0m w_bn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(bn\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m+\u001b[39m bn\u001b[38;5;241m.\u001b[39mrunning_var)))\n\u001b[0;32m    172\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mmm(w_bn, w_conv)\u001b[38;5;241m.\u001b[39mview(fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to detect faces using YOLOv8 and extract the first detected face\n",
    "def use_yolo(img_path):\n",
    "    # Assume model_yolov8 is defined and loaded somewhere in your code\n",
    "    model_yolov8=YOLO(\"./yolov8l.pt\")\n",
    "    results = model_yolov8(img_path)\n",
    "\n",
    "    # Access bounding boxes directly (assuming YOLOv8 returns a list)\n",
    "    faces = results[0].boxes\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"# No face detected\n",
    "def use_opencv(img_path):\n",
    "    image = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=1, minSize=(30, 30))\n",
    "    if len(faces) == 0:\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return \"Yes\"\n",
    "    \n",
    "\n",
    "def use_dlib(img_path): \n",
    "        image = cv2.imread(img_path)\n",
    "        gray = cv2.cvtColor(img_path, cv2.COLOR_BGR2GRAY)\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        dlib_faces = detector(gray)\n",
    "        if len(dlib_faces) == 0:\n",
    "            return  \"No\"\n",
    "        else:\n",
    "            return  \"Yes\"\n",
    "\n",
    "def save_cropped_faces(input_dir, checkpoint_file=\"checkpoint_faces.pkl\"):\n",
    "    counter=0\n",
    "    \n",
    "    yolo_path='yolov8n-face.pt'\n",
    "    model_yolov8= YOLO(yolo_path)\n",
    "\n",
    "    processed_images = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            processed_images = pickle.load(f)\n",
    "\n",
    "    # Process images recursively\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "\n",
    "        # for img_file in tqdm(files, unit=\"image\", leave=False):\n",
    "            for file in files:\n",
    "                img_path = os.path.join(root, file)\n",
    "\n",
    "                # Relative path to the input directory\n",
    "                relative_path = os.path.relpath(img_path, input_dir)\n",
    "\n",
    "                # Skip if already processed\n",
    "                if relative_path in processed_images:\n",
    "                    continue\n",
    "\n",
    "                yolo = use_yolo(img_path)\n",
    "                opencv = use_opencv(img_path)\n",
    "                # dlib = use_dlib(img_path)\n",
    "\n",
    "                    # Update processed images set\n",
    "                processed_images.add(relative_path)\n",
    "\n",
    "                    # Save checkpoint\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(processed_images, f)\n",
    "                with open('face_detection_results.csv', mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([img_path,yolo, opencv])\n",
    "\n",
    "                counter += 1\n",
    "                print(counter)\n",
    "    #     count+=len(files)\n",
    "    # print(count)\n",
    "\n",
    "input_directory = \"input_images\"\n",
    "checkpoint_filename = \"checkpoint_faces.pkl\"\n",
    "\n",
    "\n",
    "save_cropped_faces(input_directory, checkpoint_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
